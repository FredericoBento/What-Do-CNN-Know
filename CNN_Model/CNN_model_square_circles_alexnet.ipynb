{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.utils import image_dataset_from_directory\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = '../Generator/dataset1/train'\n",
    "test_dir = '../Generator/dataset1/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 500\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_SEED = random.randint(0, 1000)\n",
    "print(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    #labels='inferred',\n",
    "    #validation_split=0.2,\n",
    "    #subset=\"training\",\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    test_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    seed=RANDOM_SEED\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_batch, labels_batch in train_dataset:\n",
    "  print(image_batch.shape)\n",
    "  print(labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.1),\n",
    "    layers.RandomZoom(0.2),\n",
    "    layers.RandomBrightness(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "    layers.RandomCrop(IMG_SIZE, IMG_SIZE)\n",
    "])\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Conv2D(filters=64,kernel_size=(11,11),strides=(4,4),padding='valid',activation='relu')(x)\n",
    "# Layer 2: Max pooling layer with pool size of 3x3\n",
    "x = layers.MaxPooling2D(pool_size=(3,3),strides=(2,2))(x)\n",
    "\n",
    "# Layer 3-5: 3 more convolutional layers with similar structure as Layer 1\n",
    "x = layers.Conv2D(filters=192,kernel_size=(5,5),padding='same',activation='relu')(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3,3),strides=(2,2))(x)\n",
    "x = layers.Conv2D(filters=384,kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "x = layers.Conv2D(filters=256,kernel_size=(3,3),padding='same',activation='relu')(x)\n",
    "x = layers.MaxPooling2D(pool_size=(3,3),strides=(2,2))(x)\n",
    "\n",
    "# Layer 6: Fully connected layer with 4096 neurons\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(4096, activation=\"relu\")(x)\n",
    "\n",
    "# Layer 7: Fully connected layer with 4096 neurons\n",
    "x = layers.Dense(4096, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(4, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(), optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset, epochs=20, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.array([])\n",
    "for data_batch, label in test_dataset.take(1):\n",
    "        image = data_batch[random.randint(0,10)].numpy().astype(\"uint8\")\n",
    "        img = np.append(img, image)\n",
    "        plt.imshow(image)\n",
    "        plt.show()\n",
    "        break\n",
    "img = img.reshape(IMG_SIZE, IMG_SIZE, 3)\n",
    "img.astype(\"float32\") / 255\n",
    "img = np.expand_dims(img, axis=0)\n",
    "\n",
    "print(model.predict(img).round())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loss, val_acc = model.evaluate(test_dataset, batch_size=16)\n",
    "print(val_loss, val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_ylim([0, 1])\n",
    "plt.title('Training accuracy')\n",
    "plt.legend(loc=0)\n",
    "plt.figure()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.preprocessing.image.load_img('cats_and_dogs_small/validation/dogs/dog.1010.jpg', target_size=(150, 150), interpolation='bilinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "img_array = tf.expand_dims(img_array, 0)\n",
    "print(img_array.shape)\n",
    "result = model.predict(img_array)\n",
    "print(\"Result: \", result.round())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
